{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 3: The Decisive Test - Decomposing the Normalization Mechanism\n",
        "\n",
        "### Summary\n",
        "This notebook reproduces the third and final synthetic experiment from the paper, **\"Stability and Expression: The Dual-Mechanism of Normalization in Deep Learning.\"** This is the decisive test designed to isolate and prove the function of the two core components of our proposed dual-mechanism: the geometric constraint and the learnable affine transformation.\n",
        "\n",
        "### Hypothesis\n",
        "This experiment tests the hypothesis that normalization's success is a partnership between two parts:\n",
        "1.  **The Geometric Stabilizer:** The core act of normalizing activation vectors, which prevents representational collapse and guides the optimizer to a flat region of the loss landscape.\n",
        "2.  **The Expressive Engine:** The learnable affine parameters (gain and bias), which provide the model the freedom to find a high-performance solution *within* that stable region.\n",
        "\n",
        "To test this, we introduce a **\"Manual L2 Norm\"** model. We predict:\n",
        "- It will exhibit the **same stability** as LayerNorm/BatchNorm (flat landscape, isotropic representations) because it shares the geometric constraint.\n",
        "- It will **fail to learn the task** (high final loss) because it lacks the learnable affine parameters (the expressive engine).\n",
        "\n",
        "### Methodology\n",
        "We compare four minimalist Transformer models:\n",
        "1.  LayerNorm (Complete Mechanism)\n",
        "2.  BatchNorm (Complete Mechanism)\n",
        "3.  **Manual L2 Norm (Stabilizer Only)**\n",
        "4.  No Norm (Unconstrained Baseline)\n",
        "\n",
        "A successful outcome, where Manual L2 Norm shows stability but poor performance, would provide strong causal evidence for our two-part hypothesis."
      ],
      "metadata": {
        "id": "ad71Jf14COTR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup and Imports"
      ],
      "metadata": {
        "id": "jBAlQKDgChi6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cGgVOxMIB4UF"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Configuration\n",
        "Parameters for the synthetic experiment are defined here."
      ],
      "metadata": {
        "id": "2GL96KaXCjCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Config:\n",
        "\n",
        "    N_ENTITIES = 20\n",
        "    M_ATTRIBUTES = 20\n",
        "    VOCAB_SIZE = N_ENTITIES * M_ATTRIBUTES + 2 # +2 for PAD and MASK tokens\n",
        "    THEORETICAL_RANK = N_ENTITIES + M_ATTRIBUTES\n",
        "\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEADS = 1\n",
        "    SEQ_LENGTH = THEORETICAL_RANK + 1\n",
        "\n",
        "\n",
        "    LEARNING_RATE = 1e-4\n",
        "    BATCH_SIZE = 128\n",
        "    TRAINING_STEPS = 4000\n",
        "\n",
        "\n",
        "    LANDSCAPE_RESOLUTION = 40\n",
        "    LANDSCAPE_RANGE = 0.5\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "bvfSCECACRh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Synthetic Data Generation\n"
      ],
      "metadata": {
        "id": "aYl5GGhWCp_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def generate_associative_recall_batch(config):\n",
        "    \"\"\"Generates a batch of data for the low-rank associative recall task.\"\"\"\n",
        "    keys = torch.arange(0, config.N_ENTITIES * config.M_ATTRIBUTES)\n",
        "    X = torch.zeros((config.BATCH_SIZE, config.SEQ_LENGTH), dtype=torch.long)\n",
        "    Y = torch.zeros((config.BATCH_SIZE), dtype=torch.long)\n",
        "\n",
        "    for i in range(config.BATCH_SIZE):\n",
        "        context_indices = np.random.choice(len(keys), config.THEORETICAL_RANK, replace=False)\n",
        "        query_idx = context_indices[0]\n",
        "        context_indices = context_indices[1:]\n",
        "        sequence = torch.from_numpy(context_indices)\n",
        "        query_key = torch.tensor([query_idx])\n",
        "        X[i, :len(sequence)] = sequence\n",
        "        X[i, -1] = query_key\n",
        "        Y[i] = query_idx\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "rEH0xSKKCS-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Flexible Transformer Model\n",
        "This version of the Transformer block includes the crucial 'manual_l2' option.\n",
        "This is a non-parametric operation that only enforces the geometric constraint."
      ],
      "metadata": {
        "id": "UZn-N0vZCrbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MinimalistTransformerBlock(nn.Module):\n",
        "    \"\"\"A block supporting LayerNorm, BatchNorm, Manual L2 Norm, or none.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, normalization_type='layernorm'):\n",
        "        super().__init__()\n",
        "        self.normalization_type = normalization_type\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            self.norm1 = nn.LayerNorm(embed_dim)\n",
        "            self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        elif self.normalization_type == 'batchnorm':\n",
        "            self.norm1 = nn.BatchNorm1d(embed_dim)\n",
        "            self.norm2 = nn.BatchNorm1d(embed_dim)\n",
        "\n",
        "    def _apply_norm(self, x, norm_layer_idx):\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            norm_layer = self.norm1 if norm_layer_idx == 1 else self.norm2\n",
        "            return norm_layer(x)\n",
        "        elif self.normalization_type == 'batchnorm':\n",
        "            norm_layer = self.norm1 if norm_layer_idx == 1 else self.norm2\n",
        "            return norm_layer(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        elif self.normalization_type == 'manual_l2':\n",
        "\n",
        "            return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=1e-9)\n",
        "        return x # 'none' case\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        x = self._apply_norm(x, 1)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + ffn_output\n",
        "        x = self._apply_norm(x, 2)\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"A wrapper model with embeddings and a final prediction head.\"\"\"\n",
        "    def __init__(self, config, normalization_type='layernorm'):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(config.VOCAB_SIZE, config.EMBED_DIM)\n",
        "        self.transformer_block = MinimalistTransformerBlock(\n",
        "            config.EMBED_DIM, config.NUM_HEADS, normalization_type\n",
        "        )\n",
        "        self.to_vocab = nn.Linear(config.EMBED_DIM, config.VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embeddings(x)\n",
        "        x = self.transformer_block(x)\n",
        "        query_representation = x[:, -1, :]\n",
        "        logits = self.to_vocab(query_representation)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "Bi2B1hkECUxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Training Loop"
      ],
      "metadata": {
        "id": "JVuPrEQrCvpQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_model(config, normalization_type, device):\n",
        "    \"\"\"Trains a model and returns it along with its loss history.\"\"\"\n",
        "    print(f\"\\n--- Training model with: {normalization_type.upper()} ---\")\n",
        "    model = TransformerModel(config, normalization_type=normalization_type).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_history = []\n",
        "    for step in range(config.TRAINING_STEPS):\n",
        "        model.train()\n",
        "        X, Y = generate_associative_recall_batch(config)\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            print(f\"Step {step}/{config.TRAINING_STEPS}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, loss_history"
      ],
      "metadata": {
        "id": "6KCXCV0bCWq0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Analysis Functions (Corrected)\n",
        "The get_loss_landscape function has been corrected to define 'logits' inside the loop."
      ],
      "metadata": {
        "id": "HtkWc0MYCxT9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# global variable to store hooked activations\n",
        "activation_output = None\n",
        "\n",
        "def get_loss_landscape(model, config, device):\n",
        "    \"\"\"Computes a 1D slice of the loss landscape to visualize flatness.\"\"\"\n",
        "    norm_type_name = model.transformer_block.normalization_type.upper()\n",
        "    print(f\"Analyzing loss landscape for {norm_type_name}...\")\n",
        "    final_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    direction = [torch.randn_like(p) for p in model.parameters() if p.requires_grad]\n",
        "    norm = np.sqrt(sum(torch.sum(d**2).item() for d in direction))\n",
        "    direction = [d / norm for d in direction]\n",
        "\n",
        "    losses = []\n",
        "    alphas = np.linspace(-config.LANDSCAPE_RANGE, config.LANDSCAPE_RANGE, config.LANDSCAPE_RESOLUTION)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, Y = generate_associative_recall_batch(config) # Use a consistent test batch\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        for alpha in alphas:\n",
        "            temp_state_dict = copy.deepcopy(final_state_dict)\n",
        "            i = 0\n",
        "            for param in model.parameters():\n",
        "                if param.requires_grad:\n",
        "                    param.data += alpha * direction[i]\n",
        "                    i += 1\n",
        "\n",
        "            # *** BUG FIX: Added this line to compute logits with the perturbed model ***\n",
        "            logits = model(X)\n",
        "            losses.append(criterion(logits, Y).item())\n",
        "\n",
        "    model.load_state_dict(final_state_dict) # Restore original weights\n",
        "    return alphas, losses\n",
        "\n",
        "def get_activation_spectrum(model, config, device):\n",
        "    \"\"\"Uses a forward hook to capture and analyze activations via SVD.\"\"\"\n",
        "    norm_type_name = model.transformer_block.normalization_type.upper()\n",
        "    print(f\"Analyzing activation spectrum for {norm_type_name}...\")\n",
        "    global activation_output\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        global activation_output\n",
        "        activation_output = output.detach()\n",
        "\n",
        "    handle = model.transformer_block.register_forward_hook(hook)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, _ = generate_associative_recall_batch(config)\n",
        "        _ = model(X.to(device))\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    activations_matrix = activation_output.reshape(-1, config.EMBED_DIM)\n",
        "    _, S, _ = torch.svd(activations_matrix)\n",
        "    return S.cpu().numpy()"
      ],
      "metadata": {
        "id": "bzEiV9doCaMe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Main Execution and Data Collection\n",
        "This block runs the full experiment for all four model variants."
      ],
      "metadata": {
        "id": "mXexHbv3C08h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "results = {}\n",
        "norm_types = ['layernorm', 'batchnorm', 'manual_l2', 'none']\n",
        "\n",
        "for norm_type in norm_types:\n",
        "    config = Config()\n",
        "    if norm_type == 'none':\n",
        "        config.LEARNING_RATE = 5e-4\n",
        "\n",
        "    model, loss_history = train_model(config, normalization_type=norm_type, device=device)\n",
        "    alphas, landscape = get_loss_landscape(model, config, device)\n",
        "    spectrum = get_activation_spectrum(model, config, device)\n",
        "    results[norm_type] = {\n",
        "        \"loss\": loss_history,\n",
        "        \"alphas\": alphas,\n",
        "        \"landscape\": landscape,\n",
        "        \"spectrum\": spectrum\n",
        "    }"
      ],
      "metadata": {
        "id": "XVogoj-7Cb_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Plotting and Visualization\n",
        "This cell generates the 1x3 plot corresponding to Figure 3 in the paper."
      ],
      "metadata": {
        "id": "X1f41Sv0C5Qi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 7), sharey=False)\n",
        "fig.suptitle('The Decisive Test: Decomposing the Normalization Mechanism', fontsize=20, weight='bold')\n",
        "\n",
        "colors = {'layernorm': 'blue', 'batchnorm': 'green', 'manual_l2': 'purple', 'none': 'red'}\n",
        "labels = {\n",
        "    'layernorm':   'LayerNorm',\n",
        "    'batchnorm':   'BatchNorm',\n",
        "    'manual_l2':   'Manual L2 Norm (Pure Geometry)',\n",
        "    'none':        'No Norm (Baseline)'\n",
        "}\n",
        "linestyles = {'layernorm': '-', 'batchnorm': '--', 'manual_l2': ':', 'none': '-'}\n",
        "\n",
        "# Plot 1: Training Loss Curves\n",
        "ax = axes[0]\n",
        "for norm_type in norm_types:\n",
        "    ax.plot(results[norm_type]['loss'], label=labels[norm_type], color=colors[norm_type],\n",
        "            linestyle=linestyles[norm_type], linewidth=2.5)\n",
        "ax.set_title('Training Loss Curves', fontsize=16)\n",
        "ax.set_xlabel('Training Step', fontsize=12)\n",
        "ax.set_ylabel('Cross-Entropy Loss (Log Scale)', fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "# Plot 2: Loss Landscape Flatness\n",
        "ax = axes[1]\n",
        "for norm_type in norm_types:\n",
        "    ax.plot(results[norm_type]['alphas'], results[norm_type]['landscape'],\n",
        "            label=labels[norm_type], color=colors[norm_type], marker='o',\n",
        "            markersize=5, alpha=0.9, linestyle=linestyles[norm_type])\n",
        "ax.set_title('1D Loss Landscape Slice (Flatness)', fontsize=16)\n",
        "ax.set_xlabel('Perturbation along Random Direction (α)', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.legend(fontsize=11)\n",
        "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "\n",
        "# Plot 3: Activation Singular Value Spectrum\n",
        "ax = axes[2]\n",
        "for norm_type in norm_types:\n",
        "    ax.plot(results[norm_type]['spectrum'], label=labels[norm_type], color=colors[norm_type],\n",
        "            linestyle=linestyles[norm_type], linewidth=2.5)\n",
        "ax.set_title('Activation Singular Value Spectrum (Isotropy)', fontsize=16)\n",
        "ax.set_xlabel('Singular Value Index', fontsize=12)\n",
        "ax.set_ylabel('Singular Value Magnitude (Log Scale)', fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4ZXyjzoUCdw6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}