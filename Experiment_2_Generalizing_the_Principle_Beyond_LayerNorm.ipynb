{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 2: Generalizing the Principle Beyond LayerNorm\n",
        "\n",
        "### Summary\n",
        "This notebook reproduces the second experiment from the paper, **\"Stability and Expression: The Dual-Mechanism of Normalization in Deep Learning.\"** Having established the foundational effect of LayerNorm in the first experiment, this notebook investigates its specificity. We compare LayerNorm against Batch Normalization (BatchNorm) and a baseline with no normalization.\n",
        "\n",
        "### Hypothesis\n",
        "This experiment tests the hypothesis that the implicit regularization effect is not an idiosyncratic property of LayerNorm, but stems from a more general principle shared by other normalization layers: **the active constraint of activation statistics on an instance-by-instance basis.** We predict that:\n",
        "1.  **Equivalence in Properties:** LayerNorm and BatchNorm will exhibit nearly identical behavior. Both will converge to flat minima and maintain isotropic representations.\n",
        "2.  **Contrast with Baseline:** Both normalization methods will stand in stark contrast to the unconstrained \"No Norm\" model, which is expected to converge to a sharp minimum with collapsed representations.\n",
        "\n",
        "### Methodology\n",
        "We train three minimalist Transformer models on the synthetic \"Low-Rank Associative Recall\" task. The models differ only in their normalization strategy:\n",
        "1.  LayerNorm\n",
        "2.  BatchNorm\n",
        "3.  No Norm (Baseline)\n",
        "\n",
        "We then perform the same analysis as in Experiment 1, plotting the training loss, the loss landscape flatness, and the activation spectrum for all three models to facilitate a direct comparison."
      ],
      "metadata": {
        "id": "pM0eJc6AAss-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup and Imports"
      ],
      "metadata": {
        "id": "4sOjVHQ8BG36"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hBDrw575AWhi"
      },
      "outputs": [],
      "source": [
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Configuration\n",
        "Parameters for the synthetic experiment are defined here."
      ],
      "metadata": {
        "id": "LRlz-VJ0BJDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class Config:\n",
        "\n",
        "    N_ENTITIES = 20\n",
        "    M_ATTRIBUTES = 20\n",
        "    VOCAB_SIZE = N_ENTITIES * M_ATTRIBUTES + 2\n",
        "    THEORETICAL_RANK = N_ENTITIES + M_ATTRIBUTES\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEADS = 1\n",
        "    SEQ_LENGTH = THEORETICAL_RANK + 1\n",
        "    LEARNING_RATE = 1e-4\n",
        "    BATCH_SIZE = 128\n",
        "    TRAINING_STEPS = 4000\n",
        "    LANDSCAPE_RESOLUTION = 40\n",
        "    LANDSCAPE_RANGE = 0.5\n",
        "\n",
        "\n",
        "config = Config()"
      ],
      "metadata": {
        "id": "FqRnELgaAwx2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Synthetic Data Generation\n",
        "This function creates batches for the \"Low-Rank Associative Recall\" task."
      ],
      "metadata": {
        "id": "KxAoUVlPBQby"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_associative_recall_batch(config):\n",
        "    \"\"\"Generates a batch of data for the low-rank associative recall task.\"\"\"\n",
        "    keys = torch.arange(0, config.N_ENTITIES * config.M_ATTRIBUTES)\n",
        "    X = torch.zeros((config.BATCH_SIZE, config.SEQ_LENGTH), dtype=torch.long)\n",
        "    Y = torch.zeros((config.BATCH_SIZE), dtype=torch.long)\n",
        "\n",
        "    for i in range(config.BATCH_SIZE):\n",
        "        context_indices = np.random.choice(len(keys), config.THEORETICAL_RANK, replace=False)\n",
        "        query_idx = context_indices[0]\n",
        "        context_indices = context_indices[1:]\n",
        "        sequence = torch.from_numpy(context_indices)\n",
        "        query_key = torch.tensor([query_idx])\n",
        "        X[i, :len(sequence)] = sequence\n",
        "        X[i, -1] = query_key\n",
        "        Y[i] = query_idx\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "TdRlveuYAyOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Flexible Transformer Model\n",
        "The Transformer block is updated to support different normalization types:\n",
        "'layernorm', 'batchnorm', or 'none'. This flexibility is key to the experiment.\n"
      ],
      "metadata": {
        "id": "mdfxAmpKBTRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class MinimalistTransformerBlock(nn.Module):\n",
        "    \"\"\"A Transformer block that can use LayerNorm, BatchNorm, or no normalization.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, normalization_type='layernorm'):\n",
        "        super().__init__()\n",
        "        self.normalization_type = normalization_type\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            self.norm1 = nn.LayerNorm(embed_dim)\n",
        "            self.norm2 = nn.LayerNorm(embed_dim)\n",
        "        elif self.normalization_type == 'batchnorm':\n",
        "            self.norm1 = nn.BatchNorm1d(embed_dim)\n",
        "            self.norm2 = nn.BatchNorm1d(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            x = self.norm1(x)\n",
        "        elif self.normalization_type == 'batchnorm':\n",
        "            x = self.norm1(x.permute(0, 2, 1)).permute(0, 2, 1) # Permute for BatchNorm\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + ffn_output\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            x = self.norm2(x)\n",
        "        elif self.normalization_type == 'batchnorm':\n",
        "            x = self.norm2(x.permute(0, 2, 1)).permute(0, 2, 1)\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"A wrapper model with embeddings and a final prediction head.\"\"\"\n",
        "    def __init__(self, config, normalization_type='layernorm'):\n",
        "        super().__init__()\n",
        "        self.token_embeddings = nn.Embedding(config.VOCAB_SIZE, config.EMBED_DIM)\n",
        "        self.transformer_block = MinimalistTransformerBlock(\n",
        "            config.EMBED_DIM, config.NUM_HEADS, normalization_type\n",
        "        )\n",
        "        self.to_vocab = nn.Linear(config.EMBED_DIM, config.VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embeddings(x)\n",
        "        x = self.transformer_block(x)\n",
        "        query_representation = x[:, -1, :]\n",
        "        logits = self.to_vocab(query_representation)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "OMElSsOAA0Co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Training Loop"
      ],
      "metadata": {
        "id": "cxD2wMH4BdC9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_model(config, normalization_type, device):\n",
        "    \"\"\"Trains a model and returns it along with its loss history.\"\"\"\n",
        "    print(f\"\\n--- Training model with: {normalization_type.upper()} ---\")\n",
        "    model = TransformerModel(config, normalization_type=normalization_type).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_history = []\n",
        "    for step in range(config.TRAINING_STEPS):\n",
        "        model.train()\n",
        "        X, Y = generate_associative_recall_batch(config)\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            print(f\"Step {step}/{config.TRAINING_STEPS}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, loss_history"
      ],
      "metadata": {
        "id": "OQUGUAECA5m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Analysis Functions"
      ],
      "metadata": {
        "id": "uWPhS4wTBhum"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# global variable to store hooked activations\n",
        "activation_output = None\n",
        "\n",
        "def get_loss_landscape(model, config, device):\n",
        "    \"\"\"Computes a 1D slice of the loss landscape to visualize flatness.\"\"\"\n",
        "    norm_type_name = model.transformer_block.normalization_type.upper()\n",
        "    print(f\"Analyzing loss landscape for {norm_type_name}...\")\n",
        "    final_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "\n",
        "    direction = [torch.randn_like(p) for p in model.parameters() if p.requires_grad]\n",
        "    norm = np.sqrt(sum(torch.sum(d**2).item() for d in direction))\n",
        "    direction = [d / norm for d in direction]\n",
        "\n",
        "    losses = []\n",
        "    alphas = np.linspace(-config.LANDSCAPE_RANGE, config.LANDSCAPE_RANGE, config.LANDSCAPE_RESOLUTION)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, Y = generate_associative_recall_batch(config)\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        for alpha in alphas:\n",
        "            temp_state_dict = copy.deepcopy(final_state_dict)\n",
        "            i = 0\n",
        "\n",
        "            for param in model.parameters():\n",
        "                if param.requires_grad:\n",
        "                    param.data += alpha * direction[i]\n",
        "                    i += 1\n",
        "\n",
        "            logits = model(X)\n",
        "            losses.append(criterion(logits, Y).item())\n",
        "\n",
        "    model.load_state_dict(final_state_dict)\n",
        "    return alphas, losses\n",
        "\n",
        "def get_activation_spectrum(model, config, device):\n",
        "    \"\"\"Uses a forward hook to capture and analyze activations via SVD.\"\"\"\n",
        "    norm_type_name = model.transformer_block.normalization_type.upper()\n",
        "    print(f\"Analyzing activation spectrum for {norm_type_name}...\")\n",
        "    global activation_output\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        global activation_output\n",
        "        activation_output = output.detach()\n",
        "\n",
        "    handle = model.transformer_block.register_forward_hook(hook)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, _ = generate_associative_recall_batch(config)\n",
        "        _ = model(X.to(device))\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    activations_matrix = activation_output.reshape(-1, config.EMBED_DIM)\n",
        "    _, S, _ = torch.svd(activations_matrix)\n",
        "    return S.cpu().numpy()"
      ],
      "metadata": {
        "id": "UkPvaTPxA79b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Main Execution and Data Collection\n",
        "This block runs the full experiment for all three model variants and stores the results."
      ],
      "metadata": {
        "id": "0183obH9BkdH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "results = {}\n",
        "norm_types = ['layernorm', 'batchnorm', 'none']\n",
        "\n",
        "for norm_type in norm_types:\n",
        "\n",
        "    config = Config()\n",
        "    if norm_type == 'none':\n",
        "        config.LEARNING_RATE = 5e-4\n",
        "\n",
        "    model, loss_history = train_model(config, normalization_type=norm_type, device=device)\n",
        "    alphas, landscape = get_loss_landscape(model, config, device)\n",
        "    spectrum = get_activation_spectrum(model, config, device)\n",
        "    results[norm_type] = {\n",
        "        \"loss\": loss_history,\n",
        "        \"alphas\": alphas,\n",
        "        \"landscape\": landscape,\n",
        "        \"spectrum\": spectrum\n",
        "    }"
      ],
      "metadata": {
        "id": "8RY1nBR1A8Zh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Plotting and Visualization\n",
        "This cell generates the 1x3 plot corresponding to Figure 2 in the paper."
      ],
      "metadata": {
        "id": "tMB8WExvBogd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 7), sharey=False)\n",
        "fig.suptitle('Comparative Analysis of Normalization Techniques', fontsize=20, weight='bold')\n",
        "\n",
        "colors = {'layernorm': 'blue', 'batchnorm': 'green', 'none': 'red'}\n",
        "labels = {'layernorm': 'LayerNorm', 'batchnorm': 'BatchNorm', 'none': 'No Norm'}\n",
        "linestyles = {'layernorm': '-', 'batchnorm': (0, (3, 3)), 'none': '-'} # Solid, Dotted, Solid\n",
        "\n",
        "# Plot 1: Training Loss Curves\n",
        "ax = axes[0]\n",
        "for norm_type in norm_types:\n",
        "    ax.plot(results[norm_type]['loss'], label=labels[norm_type], color=colors[norm_type],\n",
        "            linestyle=linestyles[norm_type], linewidth=2.5)\n",
        "ax.set_title('Training Loss Curves', fontsize=16)\n",
        "ax.set_xlabel('Training Step', fontsize=12)\n",
        "ax.set_ylabel('Cross-Entropy Loss (Log Scale)', fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "# Plot 2: Loss Landscape Flatness\n",
        "ax = axes[1]\n",
        "for norm_type in norm_types:\n",
        "    ax.plot(results[norm_type]['alphas'], results[norm_type]['landscape'],\n",
        "            label=labels[norm_type], color=colors[norm_type], marker='o',\n",
        "            markersize=5, alpha=0.9, linestyle=linestyles[norm_type])\n",
        "ax.set_title('1D Loss Landscape Slice', fontsize=16)\n",
        "ax.set_xlabel('Perturbation along Random Direction (α)', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.legend(fontsize=11)\n",
        "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "\n",
        "\n",
        "# Plot 3: Activation Singular Value Spectrum\n",
        "ax = axes[2]\n",
        "for norm_type in norm_types:\n",
        "    ax.plot(results[norm_type]['spectrum'], label=labels[norm_type], color=colors[norm_type],\n",
        "            linestyle=linestyles[norm_type], linewidth=2.5)\n",
        "ax.set_title('Activation Singular Value Spectrum', fontsize=16)\n",
        "ax.set_xlabel('Singular Value Index', fontsize=12)\n",
        "ax.set_ylabel('Singular Value Magnitude (Log Scale)', fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "whnxnK1dA-Yn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}