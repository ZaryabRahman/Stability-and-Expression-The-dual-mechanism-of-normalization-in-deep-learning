{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 1: The Foundational Effect of Layer Normalization\n",
        "\n",
        "### Summary\n",
        "This notebook reproduces the foundational experiment from the paper, **\"Stability and Expression: The Dual-Mechanism of Normalization in Deep Learning.\"** We investigate the fundamental impact of Layer Normalization (LayerNorm) on the training dynamics and converged solution of a minimalist Transformer model.\n",
        "\n",
        "### Hypothesis\n",
        "The experiment tests the hypothesis that LayerNorm acts as a powerful implicit regularizer. Specifically, we expect to observe that:\n",
        "1.  **Loss Landscape:** The model with LayerNorm will converge to a wide, **flat minimum**, which is characteristic of generalizable solutions. The model without normalization will converge to a sharp, narrow minimum, indicative of memorization.\n",
        "2.  **Internal Representations:** The model with LayerNorm will maintain high-dimensional, **isotropic** internal representations. The model without normalization will suffer from **representational collapse**, where activations collapse into a low-dimensional subspace.\n",
        "\n",
        "### Methodology\n",
        "We compare two identical minimalist Transformer models trained on a synthetic \"Low-Rank Associative Recall\" task. The only difference between the models is the presence or absence of LayerNorm layers. We then analyze and plot three key aspects:\n",
        "1.  The training loss curves.\n",
        "2.  A 1D slice of the final loss landscape to measure flatness.\n",
        "3.  The singular value spectrum of the internal activations to measure isotropy."
      ],
      "metadata": {
        "id": "X8bajN4-91XQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "52AdgsRN9_JS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup and Imports"
      ],
      "metadata": {
        "id": "HF_O7wUD-EhC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ],
      "metadata": {
        "id": "6orXtWfR94F6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration\n",
        "###### All parameters for the synthetic experiment are defined here"
      ],
      "metadata": {
        "id": "nlK0qlvn-KNg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cSFcPVRF9xEW"
      },
      "outputs": [],
      "source": [
        "class Config:\n",
        "    N_ENTITIES = 20\n",
        "    M_ATTRIBUTES = 20\n",
        "    VOCAB_SIZE = N_ENTITIES * M_ATTRIBUTES + 2\n",
        "    THEORETICAL_RANK = N_ENTITIES + M_ATTRIBUTES\n",
        "\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEADS = 1\n",
        "    SEQ_LENGTH = THEORETICAL_RANK + 1\n",
        "    NUM_BLOCKS = 1\n",
        "    LEARNING_RATE = 1e-4\n",
        "    BATCH_SIZE = 128\n",
        "    TRAINING_STEPS = 4000\n",
        "    LANDSCAPE_RESOLUTION = 50\n",
        "    LANDSCAPE_RANGE = 0.5\n",
        "\n",
        "\n",
        "config = Config()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Synthetic Data Generation\n",
        "This function creates batches for the \"Low-Rank Associative Recall\" task.\n",
        "The task is designed to have a known low-rank structure, making it easy to distinguish between models that learn the underlying structure versus those that simply memorize the data."
      ],
      "metadata": {
        "id": "PHTCmqbM_bik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def generate_associative_recall_batch(config):\n",
        "    \"\"\"Generates a batch of data for the low-rank associative recall task.\"\"\"\n",
        "    keys = torch.arange(0, config.N_ENTITIES * config.M_ATTRIBUTES)\n",
        "\n",
        "    X = torch.zeros((config.BATCH_SIZE, config.SEQ_LENGTH), dtype=torch.long)\n",
        "    Y = torch.zeros((config.BATCH_SIZE), dtype=torch.long)\n",
        "\n",
        "    for i in range(config.BATCH_SIZE):\n",
        "        context_indices = np.random.choice(len(keys), config.THEORETICAL_RANK, replace=False)\n",
        "        query_idx = context_indices[0]\n",
        "        context_indices = context_indices[1:]\n",
        "\n",
        "        sequence = torch.from_numpy(context_indices)\n",
        "        query_key = torch.tensor([query_idx])\n",
        "\n",
        "        X[i, :len(sequence)] = sequence\n",
        "        X[i, -1] = query_key\n",
        "        Y[i] = query_idx\n",
        "\n",
        "    return X, Y"
      ],
      "metadata": {
        "id": "mgkkU6KG_L8T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Minimalist Transformer Model\n",
        "We define a simple Transformer with a single attention block.\n",
        "The `use_layernorm` flag allows us to instantiate two versions of this\n",
        "model: one with LayerNorm and one without."
      ],
      "metadata": {
        "id": "VOctUFcV_kye"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MinimalistTransformerBlock(nn.Module):\n",
        "    \"\"\"A single Transformer block that allows enabling/disabling LayerNorm.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, use_layernorm=True):\n",
        "        super().__init__()\n",
        "        self.use_layernorm = use_layernorm\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        if self.use_layernorm:\n",
        "            self.norm1 = nn.LayerNorm(embed_dim)\n",
        "            self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        if self.use_layernorm:\n",
        "            x = self.norm1(x)\n",
        "\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + ffn_output\n",
        "        if self.use_layernorm:\n",
        "            x = self.norm2(x)\n",
        "        return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    \"\"\"A wrapper model with embeddings and a final prediction head.\"\"\"\n",
        "    def __init__(self, config, use_layernorm=True):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(config.VOCAB_SIZE, config.EMBED_DIM)\n",
        "        self.transformer_block = MinimalistTransformerBlock(\n",
        "            config.EMBED_DIM, config.NUM_HEADS, use_layernorm\n",
        "        )\n",
        "        self.to_vocab = nn.Linear(config.EMBED_DIM, config.VOCAB_SIZE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.token_embeddings(x)\n",
        "        x = self.transformer_block(x)\n",
        "        query_representation = x[:, -1, :]\n",
        "        logits = self.to_vocab(query_representation)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "T2qEq649_Mdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Training Loop"
      ],
      "metadata": {
        "id": "_ccxxKt6_pRc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def train_model(config, use_layernorm, device):\n",
        "    \"\"\"Trains a model and returns it along with its loss history.\"\"\"\n",
        "    print(f\"\\n--- Training model {'WITH' if use_layernorm else 'WITHOUT'} LayerNorm ---\")\n",
        "    model = TransformerModel(config, use_layernorm=use_layernorm).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    loss_history = []\n",
        "    for step in range(config.TRAINING_STEPS):\n",
        "        model.train()\n",
        "        X, Y = generate_associative_recall_batch(config)\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(X)\n",
        "        loss = criterion(logits, Y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "\n",
        "        if step % 500 == 0:\n",
        "            print(f\"Step {step}/{config.TRAINING_STEPS}, Loss: {loss.item():.4f}\")\n",
        "\n",
        "    return model, loss_history"
      ],
      "metadata": {
        "id": "0pPZNCiw_OO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Analysis Functions\n",
        "These functions are used to probe the properties of the trained models.\n"
      ],
      "metadata": {
        "id": "L-Scs2M8_sGH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# global variable to store hooked activations\n",
        "activation_output = None\n",
        "\n",
        "def get_loss_landscape(model, config, device):\n",
        "    \"\"\"Computes a 1D slice of the loss landscape to visualize flatness.\"\"\"\n",
        "    print(\"Analyzing loss landscape...\")\n",
        "    final_state_dict = copy.deepcopy(model.state_dict())\n",
        "\n",
        "    direction = [torch.randn_like(p) for p in model.parameters() if p.requires_grad]\n",
        "    norm = np.sqrt(sum(torch.sum(d**2).item() for d in direction))\n",
        "    direction = [d / norm for d in direction]\n",
        "\n",
        "    losses = []\n",
        "    alphas = np.linspace(-config.LANDSCAPE_RANGE, config.LANDSCAPE_RANGE, config.LANDSCAPE_RESOLUTION)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, Y = generate_associative_recall_batch(config) # here  use a consistent test batch for landscape\n",
        "        X, Y = X.to(device), Y.to(device)\n",
        "\n",
        "        for alpha in alphas:\n",
        "            temp_state_dict = copy.deepcopy(final_state_dict)\n",
        "            i = 0\n",
        "            for name, param in temp_state_dict.items():\n",
        "                if model.get_parameter(name).requires_grad:\n",
        "                    param.data += alpha * direction[i]\n",
        "                    i += 1\n",
        "            model.load_state_dict(temp_state_dict)\n",
        "\n",
        "            logits = model(X)\n",
        "            loss = criterion(logits, Y).item()\n",
        "            losses.append(loss)\n",
        "\n",
        "    model.load_state_dict(final_state_dict)\n",
        "    return alphas, losses\n",
        "\n",
        "def get_activation_spectrum(model, config, device):\n",
        "    \"\"\"Uses a forward hook to capture and analyze activations via SVD.\"\"\"\n",
        "    print(\"Analyzing activation spectrum...\")\n",
        "    global activation_output\n",
        "\n",
        "    def hook(model, input, output):\n",
        "        global activation_output\n",
        "        activation_output = output.detach()\n",
        "\n",
        "    handle = model.transformer_block.register_forward_hook(hook)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        X, _ = generate_associative_recall_batch(config)\n",
        "        _ = model(X.to(device))\n",
        "\n",
        "    handle.remove()\n",
        "\n",
        "    activations_matrix = activation_output.reshape(-1, config.EMBED_DIM)\n",
        "    _, S, _ = torch.svd(activations_matrix)\n",
        "    return S.cpu().numpy()"
      ],
      "metadata": {
        "id": "gZqIXX4e_QYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Main Execution and Data Collection\n",
        "This block runs the full experiment for both model variants and stores the results."
      ],
      "metadata": {
        "id": "k63qnyBX_yOZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# first exp\n",
        "config = Config()\n",
        "model_ln, loss_ln = train_model(config, use_layernorm=True, device=device)\n",
        "alphas_ln, landscape_ln = get_loss_landscape(model_ln, config, device)\n",
        "spectrum_ln = get_activation_spectrum(model_ln, config, device)\n",
        "\n",
        "# second exp\n",
        "config = Config()\n",
        "\n",
        "config.LEARNING_RATE = 5e-4\n",
        "model_no_ln, loss_no_ln = train_model(config, use_layernorm=False, device=device)\n",
        "alphas_no_ln, landscape_no_ln = get_loss_landscape(model_no_ln, config, device)\n",
        "spectrum_no_ln = get_activation_spectrum(model_no_ln, config, device)"
      ],
      "metadata": {
        "id": "r2nXcQNy_SBO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Plotting and Visualization\n",
        "This cell generates the 2x2 plot corresponding to Figure 1 in the paper."
      ],
      "metadata": {
        "id": "4JWuL-YD_67e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 7), sharey=False)  # we have changed it here for clarity\n",
        "fig.suptitle(\"Analysis of Layer Normalization's Implicit Regularization\", fontsize=20, weight='bold')\n",
        "\n",
        "\n",
        "ax = axes[0]\n",
        "ax.plot(loss_ln, label='With LayerNorm', color='blue', linewidth=2)\n",
        "ax.plot(loss_no_ln, label='Without LayerNorm', color='red', linewidth=2, alpha=0.9)\n",
        "ax.set_title('Training Loss Curves', fontsize=16)\n",
        "ax.set_xlabel('Training Step', fontsize=12)\n",
        "ax.set_ylabel('Cross-Entropy Loss (Log Scale)', fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "ax = axes[1]\n",
        "ax.plot(alphas_ln, landscape_ln, label='With LayerNorm', color='blue', marker='o', markersize=5)\n",
        "ax.plot(alphas_no_ln, landscape_no_ln, label='Without LayerNorm', color='red', marker='x', markersize=5)\n",
        "ax.set_title('1D Loss Landscape Slice', fontsize=16)\n",
        "ax.set_xlabel('Perturbation along Random Direction (Î±)', fontsize=12)\n",
        "ax.set_ylabel('Loss', fontsize=12)\n",
        "ax.legend(fontsize=11)\n",
        "ax.ticklabel_format(style='sci', axis='y', scilimits=(0,0))\n",
        "\n",
        "ax = axes[2]\n",
        "ax.plot(spectrum_ln, label='With LayerNorm (Isotropic)', color='blue', linewidth=2)\n",
        "ax.plot(spectrum_no_ln, label='Without LayerNorm (Anisotropic)', color='red', linewidth=2)\n",
        "ax.set_title('Activation Singular Value Spectrum', fontsize=16)\n",
        "ax.set_xlabel('Singular Value Index', fontsize=12)\n",
        "ax.set_ylabel('Singular Value Magnitude (Log Scale)', fontsize=12)\n",
        "ax.set_yscale('log')\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, which=\"both\", ls=\"--\")\n",
        "\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "6m3WjDbf_ULA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}