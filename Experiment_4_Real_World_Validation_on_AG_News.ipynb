{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment 4: Layer-Wise Dissection of the Dual-Mechanism on a Real-World Task\n",
        "\n",
        "### Summary\n",
        "This notebook reproduces the final validation experiment from the paper, **\"Stability and Expression: The Dual-Mechanism of Normalization in Deep Learning.\"** We move beyond the controlled synthetic task to demonstrate that the proposed dual-mechanism is not an isolated phenomenon but a fundamental principle that governs the behavior of normalization in deep, practical architectures.\n",
        "\n",
        "### Hypothesis\n",
        "We validate our central hypothesis on a real-world task by comparing three deep Transformer models. We predict that:\n",
        "1.  **The Unconstrained Model (No Norm):** Will achieve high training accuracy by memorizing the data, leading to a catastrophic **cascading representational collapse** where representations become progressively lower-dimensional with depth.\n",
        "2.  **The Stabilizer-Only Model (Manual L2 Norm):** The pure geometric constraint will successfully **prevent representational collapse**, maintaining isotropic representations at all layers. However, its performance will be limited because it lacks the expressive engine.\n",
        "3.  **The Complete Mechanism (LayerNorm):** Will unite stability and expression. It will prevent collapse just like the Manual L2 model but will achieve **high performance** by using its learnable affine parameters to effectively learn the task.\n",
        "\n",
        "### Methodology\n",
        "We instantiate and train three identical 6-layer Transformer architectures on the AG News text classification benchmark. The models differ only in their normalization strategy:\n",
        "1.  **LayerNorm:** The standard, complete mechanism.\n",
        "2.  **Manual L2 Norm:** Isolates the effect of the geometric stabilizer.\n",
        "3.  **No Norm:** The unconstrained baseline.\n",
        "\n",
        "After training, we perform a layer-wise dissection by extracting the internal activations from layers 1, 3, and 6. We then plot their singular value spectra to provide direct, visual evidence of the representational quality at different depths."
      ],
      "metadata": {
        "id": "ry3VMq3UDSrZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Setup and Imports\n",
        "Install required libraries from Hugging Face for datasets and tokenizers."
      ],
      "metadata": {
        "id": "FKXp5pXmD1c6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auRGh0GQDPyW"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip install datasets transformers tqdm -q\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import copy\n",
        "from tqdm.notebook import tqdm\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Configuration\n",
        "All parameters are presented in the \"Deep Model Validation\" section of the paper.\n"
      ],
      "metadata": {
        "id": "E3uc-kXbD4s2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Config:\n",
        "\n",
        "    DATASET = \"ag_news\"\n",
        "    TOKENIZER = \"distilbert-base-uncased\"\n",
        "    MAX_LENGTH = 128\n",
        "\n",
        "\n",
        "    EMBED_DIM = 128\n",
        "    NUM_HEADS = 4\n",
        "    NUM_LAYERS = 6\n",
        "    NUM_CLASSES = 4\n",
        "\n",
        "\n",
        "    LEARNING_RATE = 2e-4\n",
        "    BATCH_SIZE = 256\n",
        "    EPOCHS = 15\n",
        "\n",
        "\n",
        "    ANALYSIS_LAYERS = [1, 3, 6]\n",
        "\n",
        "\n",
        "config = Config()\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "HrOfbcLGDc5Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Data Loading and Preprocessing\n"
      ],
      "metadata": {
        "id": "ae--VsufEEan"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(\"\\n--- Loading and Preparing AG News Dataset ---\")\n",
        "dataset = load_dataset(config.DATASET)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.TOKENIZER)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(\n",
        "        examples[\"text\"], padding=\"max_length\", truncation=True, max_length=config.MAX_LENGTH\n",
        "    )\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "tokenized_datasets.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "train_loader = DataLoader(tokenized_datasets[\"train\"], batch_size=config.BATCH_SIZE, shuffle=True)\n",
        "print(f\"Dataset prepared. Training set size: {len(tokenized_datasets['train'])} samples.\")"
      ],
      "metadata": {
        "id": "EQ5DynwHDf_W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Model Architecture\n",
        "This defines the deep Transformer encoder. The TransformerBlock is flexible\n",
        "and supports 'layernorm', 'manual_l2', or 'none' based on the experiment."
      ],
      "metadata": {
        "id": "Ciins0OdEF7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"A flexible Transformer block supporting LayerNorm, Manual L2 Norm, or none.\"\"\"\n",
        "    def __init__(self, embed_dim, num_heads, normalization_type='layernorm'):\n",
        "        super().__init__()\n",
        "        self.normalization_type = normalization_type\n",
        "        self.attention = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(embed_dim, 4 * embed_dim), nn.GELU(), nn.Linear(4 * embed_dim, embed_dim)\n",
        "        )\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            self.norm1 = nn.LayerNorm(embed_dim)\n",
        "            self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def _apply_norm(self, x, norm_layer):\n",
        "        if self.normalization_type == 'layernorm':\n",
        "            return norm_layer(x)\n",
        "        elif self.normalization_type == 'manual_l2':\n",
        "            return x / torch.norm(x, p=2, dim=-1, keepdim=True).clamp(min=1e-9)\n",
        "        return x # 'none' case\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_output, _ = self.attention(x, x, x)\n",
        "        x = x + attn_output\n",
        "        x = self._apply_norm(x, self.norm1 if hasattr(self, 'norm1') else None)\n",
        "        ffn_output = self.ffn(x)\n",
        "        x = x + ffn_output\n",
        "        x = self._apply_norm(x, self.norm2 if hasattr(self, 'norm2') else None)\n",
        "        return x\n",
        "\n",
        "class DeepTransformerEncoder(nn.Module):\n",
        "    \"\"\"The 6-layer Transformer encoder for text classification.\"\"\"\n",
        "    def __init__(self, config, normalization_type='layernorm'):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "        self.token_embeddings = nn.Embedding(tokenizer.vocab_size, config.EMBED_DIM)\n",
        "        self.layers = nn.ModuleList([\n",
        "            TransformerBlock(config.EMBED_DIM, config.NUM_HEADS, normalization_type)\n",
        "            for _ in range(config.NUM_LAYERS)\n",
        "        ])\n",
        "        self.to_output = nn.Linear(config.EMBED_DIM, config.NUM_CLASSES)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        x = self.token_embeddings(input_ids)\n",
        "        for layer in self.layers:\n",
        "            x = layer(x)\n",
        "        cls_representation = x[:, 0, :]\n",
        "        logits = self.to_output(cls_representation)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "xN30BxzVDjp6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Analysis Function for SVD\n",
        "This function uses forward hooks to extract activations from the specified\n",
        "intermediate layers of the deep model during a forward pass."
      ],
      "metadata": {
        "id": "PqfRLjOkEHi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "captured_activations = {}\n",
        "def get_activation_hook(layer_name):\n",
        "    \"\"\"Factory for hook functions to capture activations.\"\"\"\n",
        "    def hook(model, input, output):\n",
        "        captured_activations[layer_name] = output.detach()\n",
        "    return hook\n",
        "\n",
        "def get_activation_spectra(model, data_loader, layers_to_probe, device):\n",
        "    \"\"\"Runs one batch, captures activations, and computes their SVD spectra.\"\"\"\n",
        "    print(f\"  Analyzing activation spectra for layers: {layers_to_probe}...\")\n",
        "    global captured_activations\n",
        "    captured_activations = {}\n",
        "    hooks = []\n",
        "\n",
        "    for layer_idx in layers_to_probe:\n",
        "        layer_name = f\"layer_{layer_idx}\"\n",
        "        hook_handle = model.layers[layer_idx - 1].register_forward_hook(get_activation_hook(layer_name))\n",
        "        hooks.append(hook_handle)\n",
        "\n",
        "    model.eval()\n",
        "    spectra = {}\n",
        "    with torch.no_grad():\n",
        "        batch = next(iter(data_loader))\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        _ = model(input_ids)\n",
        "\n",
        "    for handle in hooks:\n",
        "        handle.remove()\n",
        "\n",
        "    for layer_name, activation_tensor in captured_activations.items():\n",
        "        activations_matrix = activation_tensor.reshape(-1, config.EMBED_DIM)\n",
        "        _, S, _ = torch.svd(activations_matrix)\n",
        "        spectra[int(layer_name.split('_')[1])] = S.cpu().numpy()\n",
        "\n",
        "    return spectra"
      ],
      "metadata": {
        "id": "3VGj1x2kDoDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Main Training and Analysis Loop"
      ],
      "metadata": {
        "id": "DIk5mwyMEMA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "results = {}\n",
        "norm_types = ['layernorm', 'manual_l2', 'none']\n",
        "\n",
        "for norm_type in norm_types:\n",
        "    print(f\"\\n--- Starting Experiment for: {norm_type.upper()} ---\")\n",
        "    model = DeepTransformerEncoder(config, normalization_type=norm_type).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    final_accuracy = 0.0\n",
        "    for epoch in tqdm(range(config.EPOCHS), desc=f\"Training {norm_type.upper()}\"):\n",
        "        model.train()\n",
        "        total_correct = 0\n",
        "        total_samples = 0\n",
        "        for batch in train_loader:\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            labels = batch[\"labels\"].to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(input_ids)\n",
        "            loss = criterion(logits, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            predictions = torch.argmax(logits, dim=1)\n",
        "            total_correct += (predictions == labels).sum().item()\n",
        "            total_samples += labels.size(0)\n",
        "        final_accuracy = total_correct / total_samples\n",
        "\n",
        "    spectra = get_activation_spectra(model, train_loader, config.ANALYSIS_LAYERS, device)\n",
        "    results[norm_type] = {\n",
        "        'accuracy': final_accuracy,\n",
        "        'spectra': spectra\n",
        "    }"
      ],
      "metadata": {
        "id": "6ULYQjwmDsGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Plotting the Results (Figure 4)"
      ],
      "metadata": {
        "id": "nqZ1xjWkENdE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n--- Generating Final Plot ---\")\n",
        "plt.style.use('seaborn-v0_8-whitegrid')\n",
        "fig, axes = plt.subplots(1, 3, figsize=(24, 7), sharey=True)\n",
        "fig.suptitle(\"Layer-Wise Dissection of Normalization in a Deep Transformer\", fontsize=22, weight='bold')\n",
        "\n",
        "colors = {'layernorm': 'blue', 'manual_l2': 'purple', 'none': 'red'}\n",
        "labels = {\n",
        "    'layernorm': f\"LayerNorm (Acc: {results['layernorm']['accuracy']:.2f})\",\n",
        "    'manual_l2': f\"Manual L2 (Acc: {results['manual_l2']['accuracy']:.2f})\",\n",
        "    'none':      f\"No Norm (Acc: {results['none']['accuracy']:.2f})\"\n",
        "}\n",
        "linestyles = {'layernorm': '-', 'manual_l2': '--', 'none': '-'}\n",
        "\n",
        "for i, layer_idx in enumerate(config.ANALYSIS_LAYERS):\n",
        "    ax = axes[i]\n",
        "    for norm_type in norm_types:\n",
        "        spectrum = results[norm_type]['spectra'][layer_idx]\n",
        "        ax.plot(\n",
        "            spectrum,\n",
        "            label=labels[norm_type],\n",
        "            color=colors[norm_type],\n",
        "            linestyle=linestyles[norm_type],\n",
        "            linewidth=2.5\n",
        "        )\n",
        "    ax.set_title(f'Activation Spectrum at Layer {layer_idx}', fontsize=16)\n",
        "    ax.set_xlabel('Singular Value Index', fontsize=12)\n",
        "    ax.set_yscale('log')\n",
        "    ax.grid(True, which=\"both\", ls=\"--\")\n",
        "    if i == 0:\n",
        "        ax.set_ylabel('Singular Value Magnitude (Log Scale)', fontsize=12)\n",
        "    ax.legend(fontsize=11)\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.93])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4_kMWH6JDuj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Print Final Training Accuracies"
      ],
      "metadata": {
        "id": "t22tTWoeEO1p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "print(\"\\n--- Final Training Accuracies ---\")\n",
        "for norm_type, data in results.items():\n",
        "    print(f\"{norm_type.upper():<12}: {data['accuracy']:.4f}\")"
      ],
      "metadata": {
        "id": "oo-TsKLWDxqp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}